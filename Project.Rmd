---
title: "Project: Retail Data Analysis"
output: html_document
date: "2024-12-11"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars}
if(!require(pacman))
  install.packages("pacman")

pacman::p_load(tidyverse, ggplot2, dplyr, cluster, arules, arulesViz, lubridate, factoextra, rpart, rpart.plot, caret, randomForest)
```

```{r}
retail <- read.csv("data/OnlineRetail.csv", stringsAsFactors = FALSE)
```

```{r}
head(retail)
```

Data Cleaning

```{r}
# Calculate missing values percentage
retail_null <- round(100 * colSums(is.na(retail)) / nrow(retail), 2)

# Display result
retail_null
```

```{r}
# Remove rows with any missing values
retail_clean <- na.omit(retail)

# Check the structure of the cleaned dataset
str(retail_clean)
```

Data Preparation

```{r}
# Convert InvoiceDate to a datetime object
retail_clean$InvoiceDate <- dmy_hm(retail_clean$InvoiceDate)

# Check the structure of the InvoiceDate column
str(retail_clean$InvoiceDate)

# Preview the first few rows
head(retail_clean$InvoiceDate)
```

```{r}
# Create Amount column
retail_clean <- retail_clean %>%
  mutate(Amount = Quantity * UnitPrice)

# Calculate Monetary
rfm_m <- retail_clean %>%
  group_by(CustomerID) %>%
  summarise(Amount = sum(Amount, na.rm = TRUE))

# Calculate Frequency
rfm_f <- retail_clean %>%
  group_by(CustomerID) %>%
  summarise(Frequency = n_distinct(InvoiceNo))

# Calculate Recency
rfm_r <- retail_clean %>%
  group_by(CustomerID) %>%
  summarise(LastPurchase = max(InvoiceDate, na.rm = TRUE)) %>%
  mutate(Recency = as.numeric(difftime(Sys.Date(), as.Date(LastPurchase), units = "days")))

# Combine RFM metrics
rfm <- rfm_m %>%
  inner_join(rfm_f, by = "CustomerID") %>%
  inner_join(rfm_r, by = "CustomerID") %>%
  select(CustomerID, Recency, Frequency, Amount)

# Merge RFM metrics back to the original dataset
retail_clean <- retail_clean %>%
  left_join(rfm, by = "CustomerID")

# Preview the final dataset
head(retail_clean)
```

```{r}
# Rename Amount.x to Amount and Amount.y to Monetary
retail_clean <- retail_clean %>%
  rename(
    Amount = Amount.x,
    Monetary = Amount.y
  )

# Check the structure to confirm the changes
str(retail_clean)
```

```{r}
# Compute the maximum date from the dataset
max_date <- max(retail_clean$InvoiceDate, na.rm = TRUE)

# Compute the difference between the max date and each transaction date
retail_clean <- retail_clean %>%
  mutate(Diff = as.numeric(difftime(max_date, InvoiceDate, units = "days")))

# Preview the updated dataset
head(retail_clean)
```

```{r}
# Convert InvoiceDate to a datetime object
retail_clean$InvoiceDate <- as.POSIXct(retail_clean$InvoiceDate)

# Calculate the difference in days
retail_clean <- retail_clean %>%
  mutate(Diff = as.integer(difftime(max_date, InvoiceDate, units = "days")))
```

Exploratory Data Analysis

1.  Transaction Volume Over Time

```{r}
transactions_by_date <- retail_clean %>%
  mutate(InvoiceDate = as.Date(InvoiceDate)) %>%
  group_by(InvoiceDate) %>%
  summarise(TransactionVolume = n())

# Plot transaction volume over time
ggplot(transactions_by_date, aes(x = InvoiceDate, y = TransactionVolume)) +
  geom_line() +
  labs(
    title = "Transaction Volume Over Time",
    x = "2011",
    y = "Number of Transactions"
  ) +
  scale_x_date(date_labels = "%b", date_breaks = "1 month") +
  theme_minimal()
```
-	Transaction volume exhibits a steady increase starting from mid-year, peaking in the months leading up to December, likely due to holiday shopping.
-	A notable dip is observed in January, which could be attributed to post-holiday season slowdown.

2.  Revenue Over Time

```{r}
# Aggregate revenue by date
revenue_by_date <- retail_clean %>%
  mutate(InvoiceDate = as.Date(InvoiceDate)) %>%
  group_by(InvoiceDate) %>%
  summarise(DailyRevenue = sum(Amount, na.rm = TRUE))

# Plot revenue over time
ggplot(revenue_by_date, aes(x = InvoiceDate, y = DailyRevenue)) +
  geom_line(color = "blue") +
  labs(
    title = "Revenue Over Time",
    x = "2011",
    y = "Revenue"
  ) +
  scale_x_date(date_labels = "%b", date_breaks = "1 month") +
  theme_minimal()
```
	•	Revenue shows a fluctuating pattern, with notable peaks observed in the months of September and December, likely driven by seasonal sales events.
	•	A sharp dip in revenue is evident during January, aligning with post-holiday shopping slowdown.

3.  Hourly Transaction Patterns

```{r}
library(lubridate)

# Ensure 'Hour' is treated as a proper datetime for formatting
transactions_by_hour <- hourly_transactions %>%
  mutate(Time = ymd_h(paste("2011-01-01", Hour, sep = " ")))  # Use a dummy date

# Plot with time-based x-axis
ggplot(transactions_by_hour, aes(x = Time, y = TransactionCount)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(
    title = "Hourly Transaction Patterns",
    x = "Time of the Day",
    y = "Total Transactions"
  ) +
  scale_x_datetime(
    date_labels = "%H:%M",  # Format as HH:MM (e.g., 00:00, 12:00)
    date_breaks = "1 hour"  # Show tick marks every hour
  ) +
  theme_minimal()
```
	•	Transactions peak during midday hours, particularly between 11:00 AM and 2:00 PM, suggesting a high level of customer activity during this period.
	•	Early morning (before 9:00 AM) and late evening (after 6:00 PM) show significantly lower transaction volumes, indicating reduced business activity during these hours.

4.  Day of the Week Analysis

```{r}
# Extract day of the week
weekday_transactions <- retail_clean %>%
  mutate(Weekday = lubridate::wday(InvoiceDate, label = TRUE)) %>%
  group_by(Weekday) %>%
  summarise(TransactionCount = n())

# Plot transactions by weekday
ggplot(weekday_transactions, aes(x = Weekday, y = TransactionCount)) +
  geom_bar(stat = "identity", fill = "coral") +
  labs(
    title = "Transactions by Day of the Week",
    x = "Day of the Week",
    y = "Number of Transactions"
  ) +
  theme_minimal()
```
	•	Transaction volume is highest on Thursdays and lowest on Sundays, indicating a peak in business activity towards the end of the week.
	•	Weekends (Saturday and Sunday) exhibit lower transaction volumes compared to weekdays, suggesting reduced business operations or customer activity during this period.


5.  Monthly Revenue by Month

```{r}
# Aggregate revenue by month
monthly_revenue <- retail_clean %>%
  mutate(Month = lubridate::floor_date(InvoiceDate, unit = "month")) %>%
  group_by(Month) %>%
  summarise(MonthlyRevenue = sum(Amount, na.rm = TRUE))

# Plot monthly revenue trends
ggplot(monthly_revenue, aes(x = Month, y = MonthlyRevenue)) +
  geom_line(color = "green") +
  labs(
    title = "Monthly Revenue Trends",
    x = "Month",
    y = "Revenue in $"
  ) +
  theme_minimal()
```
  •	Revenue shows a general upward trend over the year, with notable peaks in the months of September and December, likely driven by seasonal sales events.
	•	A significant dip in revenue is observed in January, aligning with post-holiday shopping slowdown.

6.  Heatmap of Hourly and Weekly Patterns

```{r}
# Create a heatmap of transactions by hour and weekday
heatmap_data <- retail_clean %>%
  mutate(
    Hour = lubridate::hour(InvoiceDate),
    Weekday = lubridate::wday(InvoiceDate, label = TRUE)
  ) %>%
  group_by(Weekday, Hour) %>%
  summarise(TransactionCount = n())

# Plot heatmap
ggplot(heatmap_data, aes(x = Hour, y = Weekday, fill = TransactionCount)) +
  geom_tile() +
  labs(
    title = "Heatmap of Transactions by Hour and Day of the Week",
    x = "Hour of the Day",
    y = "Day of the Week",
    fill = "Transactions"
  ) +
  theme_minimal()
```
	•	The heatmap reveals distinct patterns in transaction volume based on the hour of the day and the day of the week.
	•	Transactions are concentrated during midday hours (11:00 AM to 2:00 PM) on weekdays, with lower activity observed during early morning and late evening hours.
	•	Weekends (Saturday and Sunday) exhibit lower transaction volumes compared to weekdays, particularly during early morning and late evening hours.

CLUSTERING ALGORITHMS - K MEANS AND HIERARCHIAL

```{r}
# retail_clean
# sapply(retail_clean, class)

# 1. K Means Clustering

# Load necessary libraries
library(ggplot2)
library(dplyr)
library(cluster)
library(factoextra) # For better silhouette visualization

# Load your dataset
# retail_clean <- read.csv("path/to/retail_clean.csv")

# Select the RFM columns for clustering
rfm_data <- retail_clean %>%
  select(Recency, Frequency, Monetary)

# Take a sample of the data
set.seed(123) # For reproducibility
sample_size <- 7000 # Adjust the sample size as needed
sample_indices <- sample(1:nrow(rfm_data), size = sample_size)
rfm_sample <- rfm_data[sample_indices, ]

# Scale the sampled data
rfm_scaled <- scale(rfm_sample)

# Determine the optimal number of clusters using the Elbow Method
wcss <- vector()
for (i in 1:10) {
  kmeans_result <- kmeans(rfm_scaled, centers = i, nstart = 25)
  wcss[i] <- kmeans_result$tot.withinss
}

# Plot the Elbow Method
plot(1:10, wcss, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of clusters K",
     ylab = "Total within-clusters sum of squares")

# Perform k-means clustering with the chosen number of clusters
optimal_clusters <- 3 # Choose based on the Elbow plot
kmeans_result <- kmeans(rfm_scaled, centers = optimal_clusters, nstart = 25)

# Add the cluster assignments to the sampled data
rfm_sample$Cluster <- as.factor(kmeans_result$cluster)

# Calculate silhouette score
silhouette_score <- silhouette(kmeans_result$cluster, dist(rfm_scaled))

# Visualize silhouette scores using factoextra
fviz_silhouette(silhouette_score) +
  labs(title = "Silhouette Plot for K-Means Clustering")

# Calculate average silhouette width
avg_silhouette_width <- mean(silhouette_score[, 3])
cat("Average Silhouette Width:", avg_silhouette_width, "\n")

# Visualize the clusters using ggplot2
ggplot(rfm_sample, aes(x = Recency, y = Frequency, color = Cluster)) +
  geom_point(alpha = 0.6, size = 3) +
  labs(title = "K-Means Clustering of Retail Data (Sample)",
       x = "Recency",
       y = "Frequency") +
  theme_minimal()
```




```{r}

# 2. Hierarchial  Clustering

# Load necessary libraries
library(ggplot2)
library(dplyr)
library(cluster)
library(factoextra) # For visualization

# Load your dataset
# retail_clean <- read.csv("path/to/retail_clean.csv")

# Select the RFM columns for clustering
rfm_data <- retail_clean %>%
  select(Recency, Frequency, Monetary)

# Take a sample of the data
set.seed(123) # For reproducibility
sample_size <- 7000 # Adjust the sample size as needed
sample_indices <- sample(1:nrow(rfm_data), size = sample_size)
rfm_sample <- rfm_data[sample_indices, ]

# Scale the sampled data
rfm_scaled <- scale(rfm_sample)

# Compute the distance matrix
dist_matrix <- dist(rfm_scaled, method = "euclidean")

# Perform hierarchical clustering using Ward's method
hc_result <- hclust(dist_matrix, method = "ward.D2")

# Plot the dendrogram
plot(hc_result, labels = FALSE, hang = -1, main = "Dendrogram of Hierarchical Clustering")

# Cut the dendrogram into a desired number of clusters
optimal_clusters <- 3 # Adjust based on the dendrogram
cluster_assignments <- cutree(hc_result, k = optimal_clusters)

# Add the cluster assignments to the sampled data
rfm_sample$Cluster <- as.factor(cluster_assignments)

# Calculate silhouette score
silhouette_score <- silhouette(cluster_assignments, dist_matrix)

# Visualize silhouette scores using factoextra
fviz_silhouette(silhouette_score) +
  labs(title = "Silhouette Plot for Hierarchical Clustering")

# Calculate average silhouette width
avg_silhouette_width <- mean(silhouette_score[, 3])
cat("Average Silhouette Width:", avg_silhouette_width, "\n")

# Visualize the clusters using ggplot2
ggplot(rfm_sample, aes(x = Recency, y = Frequency, color = Cluster)) +
  geom_point(alpha = 0.6, size = 3) +
  labs(title = "Hierarchical Clustering of Retail Data (Sample)",
       x = "Recency",
       y = "Frequency") +
  theme_minimal()

```

ASSOCIATION ALGORITHM

```{r}
# 1. ECLAT Algorithm

# Load necessary libraries
library(arules)
library(ggplot2)

# Convert the dataset to a transaction format
transactions <- as(split(retail_clean$StockCode, retail_clean$InvoiceNo), "transactions")

# Apply the Eclat algorithm to find frequent itemsets
eclat_result <- eclat(transactions, parameter = list(supp = 0.01, maxlen = 5))

# Inspect the frequent itemsets
inspect(eclat_result)

# Convert the result to a data frame for visualization
itemsets_df <- as(eclat_result, "data.frame")

# Sort by support and take the top 10 itemsets
top_itemsets <- head(itemsets_df[order(-itemsets_df$support), ], 10)

# Visualize the top frequent itemsets using ggplot2
ggplot(top_itemsets, aes(x = reorder(items, support), y = support)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 10 Frequent Itemsets",
       x = "Itemsets",
       y = "Support") +
  theme_minimal()
```
```{r}
library(dplyr)

# Convert the Eclat result to a data frame
itemsets_df <- as(eclat_result, "data.frame") %>%
  mutate(
    items = as.character(items),       # Convert itemsets to character
    support = round(as.numeric(support), 4)  # Ensure numeric and round support
  )

# Split the itemsets into individual StockCodes
itemsets_with_desc <- itemsets_df %>%
  rowwise() %>%
  mutate(
    Descriptions = paste(
      unlist(strsplit(items, ",")) %>%
        gsub("[{}]", "", .) %>%                   # Remove curly braces
        trimws() %>%                              # Trim whitespace
        sapply(function(code) {                  # Map each StockCode to its description
          desc <- retail_clean %>%
            filter(StockCode == code) %>%
            slice(1) %>%                          # Take the first matching description
            pull(Description)
          ifelse(length(desc) > 0, desc, code)   # Return description or StockCode if no match
        }),
      collapse = ", "                            # Combine descriptions into one string
    )
  ) %>%
  select(items, Descriptions, support)           # Keep the necessary columns

# Display the resulting data frame
View(itemsets_with_desc)
```



```{r}

# 2. FP Growth

# Load necessary libraries
library(arules)
library(arulesViz)

# Load your dataset and convert to transactions
# retail_data <- read.csv("path/to/retail_clean.csv")
transactions <- as(split(retail_clean$StockCode, retail_clean$InvoiceNo), "transactions")

# Apply the FP-Growth algorithm to find association rules
rules <- apriori(transactions, parameter = list(supp = 0.01, conf = 0.8))

# Inspect the rules
inspect(rules)

# Sort and inspect the top rules by lift
top_rules <- sort(rules, by = "lift")[1:10]
inspect(top_rules)

# Plot a matrix of rules
plot(rules, method = "matrix", measure = c("support", "confidence"), shading = "lift")

# Visualize the rules using arulesViz
plot(top_rules, method = "graph", measure = "support", shading = "lift")
```

| LHS Description                         | RHS Description                     | Support  | Confidence | Coverage  | Lift    | Count |
|-----------------------------------------|-------------------------------------|----------|------------|-----------|---------|-------|
| WHITE METAL LANTERN                     | KNITTED UNION FLAG HOT WATER BOTTLE | 0.010635 | 0.828      | 0.012844  | 56.538  | 236   |
| CREAM CUPID HEARTS COAT HANGER          | RED WOOLLY HOTTIE WHITE HEART       | 0.010455 | 0.844      | 0.012393  | 55.060  | 232   |
| GLASS STAR FROSTED T-LIGHT HOLDER       | SET 7 BABUSHKA NESTING BOXES        | 0.010230 | 0.819      | 0.012483  | 41.708  | 227   |
| ASSORTED COLOUR BIRD ORNAMENT + PEARS   | PAINTED METAL PEARS ASSORTED        | 0.017891 | 0.843      | 0.021226  | 22.373  | 397   |
| PAINTED METAL PEARS ASSORTED + LAMP     | ASSORTED COLOUR BIRD ORNAMENT       | 0.017891 | 0.880      | 0.020324  | 26.648  | 397   |
| TEA PLATE + VINTAGE CUP                 | LAMP ANTIQUE                        | 0.012348 | 0.859      | 0.014376  | 26.002  | 274   |
| TEA PLATE + LAMP                        | VINTAGE CUP                         | 0.012213 | 0.850      | 0.014376  | 22.549  | 271   |
| METAL CUP + VINTAGE PLATE               | LAMP ANTIQUE                        | 0.014241 | 0.825      | 0.017260  | 21.900  | 316   |
| METAL CUP + VINTAGE PLATE + LAMP        | LAMP ANTIQUE                        | 0.010861 | 0.880      | 0.012348  | 23.346  | 241   |
| METAL CUP + VINTAGE PLATE + LAMP COMBO  | TEA PLATE                           | 0.010861 | 0.889      | 0.012213  | 26.922  | 241   |



Classification Algorithms

1. Decision Tree for Transaction Classification
```{r}
# Load necessary libraries
library(dplyr)
library(rpart)
library(rpart.plot)

# Step 1: Preprocess the data
# Create a target variable: classify transactions as "High" or "Low" based on Amount
retail_clean <- retail_clean %>%
  mutate(
    AmountCategory = ifelse(Amount > median(Amount, na.rm = TRUE), "High", "Low")
  )

# Convert AmountCategory to a factor
retail_clean$AmountCategory <- as.factor(retail_clean$AmountCategory)

# Step 2: Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(retail_clean), 0.7 * nrow(retail_clean))
train_data <- retail_clean[train_indices, ]
test_data <- retail_clean[-train_indices, ]

# Step 3: Build the classification model
model <- rpart(
  AmountCategory ~ Quantity + UnitPrice + Recency + Frequency + Monetary,
  data = train_data,
  method = "class"
)

# Step 4: Visualize the decision tree
rpart.plot(model, main = "Decision Tree for Transaction Classification")

# Step 5: Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "class")

# Step 6: Evaluate the model
confusion_matrix <- table(Predicted = predictions, Actual = test_data$AmountCategory)
print(confusion_matrix)

# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Model Accuracy:", accuracy, "\n")
```

```{r}
# Step 5: Make predictions on the test data
predictions <- predict(model, newdata = test_data, type = "class")

# Step 6: Generate a confusion matrix
confusion_matrix <- table(Predicted = predictions, Actual = test_data$AmountCategory)
print("Confusion Matrix:")
print(confusion_matrix)

# Step 7: Calculate evaluation metrics
# Accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy:", accuracy, "\n")

# Precision for "High" category
precision_high <- confusion_matrix["High", "High"] / sum(confusion_matrix["High", ])
cat("Precision (High):", precision_high, "\n")

# Recall (Sensitivity) for "High" category
recall_high <- confusion_matrix["High", "High"] / sum(confusion_matrix[, "High"])
cat("Recall (High):", recall_high, "\n")

# F1-Score for "High" category
f1_high <- 2 * (precision_high * recall_high) / (precision_high + recall_high)
cat("F1-Score (High):", f1_high, "\n")

# Precision for "Low" category
precision_low <- confusion_matrix["Low", "Low"] / sum(confusion_matrix["Low", ])
cat("Precision (Low):", precision_low, "\n")

# Recall (Sensitivity) for "Low" category
recall_low <- confusion_matrix["Low", "Low"] / sum(confusion_matrix[, "Low"])
cat("Recall (Low):", recall_low, "\n")

# F1-Score for "Low" category
f1_low <- 2 * (precision_low * recall_low) / (precision_low + recall_low)
cat("F1-Score (Low):", f1_low, "\n")
```

2. Random Forest for Customer Segmentation








