---
title: "Project"
output: html_document
date: "2024-12-11"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars}
if(!require(pacman))
  install.packages("pacman")

pacman::p_load(tidyverse, ggplot2, dplyr, cluster, arules, arulesViz, lubridate)
```

```{r}
retail <- read.csv("data/OnlineRetail.csv", stringsAsFactors = FALSE)
```

```{r}
head(retail)
```

Data Cleaning

```{r}
# Calculate missing values percentage
retail_null <- round(100 * colSums(is.na(retail)) / nrow(retail), 2)

# Display result
retail_null
```

```{r}
# Remove rows with any missing values
retail_clean <- na.omit(retail)

# Check the structure of the cleaned dataset
str(retail_clean)
```

Data Preparation

```{r}
# Convert InvoiceDate to a datetime object
retail_clean$InvoiceDate <- dmy_hm(retail_clean$InvoiceDate)

# Check the structure of the InvoiceDate column
str(retail_clean$InvoiceDate)

# Preview the first few rows
head(retail_clean$InvoiceDate)
```

```{r}
# Create Amount column
retail_clean <- retail_clean %>%
  mutate(Amount = Quantity * UnitPrice)

# Calculate Monetary
rfm_m <- retail_clean %>%
  group_by(CustomerID) %>%
  summarise(Amount = sum(Amount, na.rm = TRUE))

# Calculate Frequency
rfm_f <- retail_clean %>%
  group_by(CustomerID) %>%
  summarise(Frequency = n_distinct(InvoiceNo))

# Calculate Recency
rfm_r <- retail_clean %>%
  group_by(CustomerID) %>%
  summarise(LastPurchase = max(InvoiceDate, na.rm = TRUE)) %>%
  mutate(Recency = as.numeric(difftime(Sys.Date(), as.Date(LastPurchase), units = "days")))

# Combine RFM metrics
rfm <- rfm_m %>%
  inner_join(rfm_f, by = "CustomerID") %>%
  inner_join(rfm_r, by = "CustomerID") %>%
  select(CustomerID, Recency, Frequency, Amount)

# Merge RFM metrics back to the original dataset
retail_clean <- retail_clean %>%
  left_join(rfm, by = "CustomerID")

# Preview the final dataset
head(retail_clean)
```

```{r}
# Rename Amount.x to Amount and Amount.y to Monetary
retail_clean <- retail_clean %>%
  rename(
    Amount = Amount.x,
    Monetary = Amount.y
  )

# Check the structure to confirm the changes
str(retail_clean)
```

```{r}
# Compute the maximum date from the dataset
max_date <- max(retail_clean$InvoiceDate, na.rm = TRUE)

# Compute the difference between the max date and each transaction date
retail_clean <- retail_clean %>%
  mutate(Diff = as.numeric(difftime(max_date, InvoiceDate, units = "days")))

# Preview the updated dataset
head(retail_clean)
```

```{r}
# Convert InvoiceDate to a datetime object
retail_clean$InvoiceDate <- as.POSIXct(retail_clean$InvoiceDate)

# Calculate the difference in days
retail_clean <- retail_clean %>%
  mutate(Diff = as.integer(difftime(max_date, InvoiceDate, units = "days")))
```

Exploratory Data Analysis

1.  Transaction Volume Over Time

```{r}
transactions_by_date <- retail_clean %>%
  mutate(InvoiceDate = as.Date(InvoiceDate)) %>%
  group_by(InvoiceDate) %>%
  summarise(TransactionVolume = n())

# Plot transaction volume over time
ggplot(transactions_by_date, aes(x = InvoiceDate, y = TransactionVolume)) +
  geom_line() +
  labs(
    title = "Transaction Volume Over Time",
    x = "Date",
    y = "Number of Transactions"
  ) +
  theme_minimal()
```

2.  Revenue Over Time

```{r}
# Aggregate revenue by date
revenue_by_date <- retail_clean %>%
  mutate(InvoiceDate = as.Date(InvoiceDate)) %>%
  group_by(InvoiceDate) %>%
  summarise(DailyRevenue = sum(Amount, na.rm = TRUE))

# Plot revenue over time
ggplot(revenue_by_date, aes(x = InvoiceDate, y = DailyRevenue)) +
  geom_line(color = "blue") +
  labs(
    title = "Daily Revenue Over Time",
    x = "Date",
    y = "Revenue"
  ) +
  theme_minimal()
```

3.  Hourly Transaction Patterns

```{r}
# Extract hour from InvoiceDate
hourly_transactions <- retail_clean %>%
  mutate(Hour = lubridate::hour(InvoiceDate)) %>%
  group_by(Hour) %>%
  summarise(TransactionCount = n())

# Plot hourly transaction patterns
ggplot(hourly_transactions, aes(x = Hour, y = TransactionCount)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(
    title = "Hourly Transaction Patterns",
    x = "Hour of the Day",
    y = "Number of Transactions"
  ) +
  theme_minimal()
```

4.  Day of the Week Analysis

```{r}
# Extract day of the week
weekday_transactions <- retail_clean %>%
  mutate(Weekday = lubridate::wday(InvoiceDate, label = TRUE)) %>%
  group_by(Weekday) %>%
  summarise(TransactionCount = n())

# Plot transactions by weekday
ggplot(weekday_transactions, aes(x = Weekday, y = TransactionCount)) +
  geom_bar(stat = "identity", fill = "coral") +
  labs(
    title = "Transactions by Day of the Week",
    x = "Day of the Week",
    y = "Number of Transactions"
  ) +
  theme_minimal()
```

5.  Monthly Revenue by Month

```{r}
# Aggregate revenue by month
monthly_revenue <- retail_clean %>%
  mutate(Month = lubridate::floor_date(InvoiceDate, unit = "month")) %>%
  group_by(Month) %>%
  summarise(MonthlyRevenue = sum(Amount, na.rm = TRUE))

# Plot monthly revenue trends
ggplot(monthly_revenue, aes(x = Month, y = MonthlyRevenue)) +
  geom_line(color = "green") +
  labs(
    title = "Monthly Revenue Trends",
    x = "Month",
    y = "Revenue in $"
  ) +
  theme_minimal()
```

6.  Heatmap of Hourly and Weekly Patterns

```{r}
# Create a heatmap of transactions by hour and weekday
heatmap_data <- retail_clean %>%
  mutate(
    Hour = lubridate::hour(InvoiceDate),
    Weekday = lubridate::wday(InvoiceDate, label = TRUE)
  ) %>%
  group_by(Weekday, Hour) %>%
  summarise(TransactionCount = n())

# Plot heatmap
ggplot(heatmap_data, aes(x = Hour, y = Weekday, fill = TransactionCount)) +
  geom_tile() +
  labs(
    title = "Heatmap of Transactions by Hour and Day of the Week",
    x = "Hour of the Day",
    y = "Day of the Week",
    fill = "Transactions"
  ) +
  theme_minimal()
```


CLUSTERING ALGORITHMS - K MEANS AND HIERARCHIAL

```{r}
# retail_clean
# sapply(retail_clean, class)

# 1. K Means Clustering

# Load necessary libraries
library(ggplot2)
library(dplyr)
library(cluster)
library(factoextra) # For better silhouette visualization

# Load your dataset
# retail_clean <- read.csv("path/to/retail_clean.csv")

# Select the RFM columns for clustering
rfm_data <- retail_clean %>%
  select(Recency, Frequency, Monetary)

# Take a sample of the data
set.seed(123) # For reproducibility
sample_size <- 7000 # Adjust the sample size as needed
sample_indices <- sample(1:nrow(rfm_data), size = sample_size)
rfm_sample <- rfm_data[sample_indices, ]

# Scale the sampled data
rfm_scaled <- scale(rfm_sample)

# Determine the optimal number of clusters using the Elbow Method
wcss <- vector()
for (i in 1:10) {
  kmeans_result <- kmeans(rfm_scaled, centers = i, nstart = 25)
  wcss[i] <- kmeans_result$tot.withinss
}

# Plot the Elbow Method
plot(1:10, wcss, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of clusters K",
     ylab = "Total within-clusters sum of squares")

# Perform k-means clustering with the chosen number of clusters
optimal_clusters <- 3 # Choose based on the Elbow plot
kmeans_result <- kmeans(rfm_scaled, centers = optimal_clusters, nstart = 25)

# Add the cluster assignments to the sampled data
rfm_sample$Cluster <- as.factor(kmeans_result$cluster)

# Calculate silhouette score
silhouette_score <- silhouette(kmeans_result$cluster, dist(rfm_scaled))

# Visualize silhouette scores using factoextra
fviz_silhouette(silhouette_score) +
  labs(title = "Silhouette Plot for K-Means Clustering")

# Calculate average silhouette width
avg_silhouette_width <- mean(silhouette_score[, 3])
cat("Average Silhouette Width:", avg_silhouette_width, "\n")

# Visualize the clusters using ggplot2
ggplot(rfm_sample, aes(x = Recency, y = Frequency, color = Cluster)) +
  geom_point(alpha = 0.6, size = 3) +
  labs(title = "K-Means Clustering of Retail Data (Sample)",
       x = "Recency",
       y = "Frequency") +
  theme_minimal()
```




```{r}

# 2. Hierarchial  Clustering

# Load necessary libraries
library(ggplot2)
library(dplyr)
library(cluster)
library(factoextra) # For visualization

# Load your dataset
# retail_clean <- read.csv("path/to/retail_clean.csv")

# Select the RFM columns for clustering
rfm_data <- retail_clean %>%
  select(Recency, Frequency, Monetary)

# Take a sample of the data
set.seed(123) # For reproducibility
sample_size <- 7000 # Adjust the sample size as needed
sample_indices <- sample(1:nrow(rfm_data), size = sample_size)
rfm_sample <- rfm_data[sample_indices, ]

# Scale the sampled data
rfm_scaled <- scale(rfm_sample)

# Compute the distance matrix
dist_matrix <- dist(rfm_scaled, method = "euclidean")

# Perform hierarchical clustering using Ward's method
hc_result <- hclust(dist_matrix, method = "ward.D2")

# Plot the dendrogram
plot(hc_result, labels = FALSE, hang = -1, main = "Dendrogram of Hierarchical Clustering")

# Cut the dendrogram into a desired number of clusters
optimal_clusters <- 3 # Adjust based on the dendrogram
cluster_assignments <- cutree(hc_result, k = optimal_clusters)

# Add the cluster assignments to the sampled data
rfm_sample$Cluster <- as.factor(cluster_assignments)

# Calculate silhouette score
silhouette_score <- silhouette(cluster_assignments, dist_matrix)

# Visualize silhouette scores using factoextra
fviz_silhouette(silhouette_score) +
  labs(title = "Silhouette Plot for Hierarchical Clustering")

# Calculate average silhouette width
avg_silhouette_width <- mean(silhouette_score[, 3])
cat("Average Silhouette Width:", avg_silhouette_width, "\n")

# Visualize the clusters using ggplot2
ggplot(rfm_sample, aes(x = Recency, y = Frequency, color = Cluster)) +
  geom_point(alpha = 0.6, size = 3) +
  labs(title = "Hierarchical Clustering of Retail Data (Sample)",
       x = "Recency",
       y = "Frequency") +
  theme_minimal()

```

ASSOCIATION ALGORITHM

```{r}
# 1. ECLAT Algorithm

# Load necessary libraries
library(arules)
library(ggplot2)

# Load your dataset
# retail_data <- read.csv("path/to/retail_clean.csv")

# Convert the dataset to a transaction format
transactions <- as(split(retail_clean$StockCode, retail_clean$InvoiceNo), "transactions")

# Apply the Eclat algorithm to find frequent itemsets
eclat_result <- eclat(transactions, parameter = list(supp = 0.01, maxlen = 5))

# Inspect the frequent itemsets
inspect(eclat_result)

# Convert the result to a data frame for visualization
itemsets_df <- as(eclat_result, "data.frame")

# Sort by support and take the top 10 itemsets
top_itemsets <- head(itemsets_df[order(-itemsets_df$support), ], 10)

# Visualize the top frequent itemsets using ggplot2
ggplot(top_itemsets, aes(x = reorder(items, support), y = support)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 10 Frequent Itemsets",
       x = "Itemsets",
       y = "Support") +
  theme_minimal()

```


```{r}

# 2. FP Growth

# Load necessary libraries
library(arules)
library(arulesViz)

# Load your dataset and convert to transactions
# retail_data <- read.csv("path/to/retail_clean.csv")
transactions <- as(split(retail_clean$StockCode, retail_clean$InvoiceNo), "transactions")

# Apply the FP-Growth algorithm to find association rules
rules <- apriori(transactions, parameter = list(supp = 0.01, conf = 0.8))

# Inspect the rules
inspect(rules)

# Sort and inspect the top rules by lift
top_rules <- sort(rules, by = "lift")[1:10]
inspect(top_rules)

# Visualize the rules using arulesViz
plot(top_rules, method = "graph", measure = "support", shading = "lift")

# Plot a matrix of rules
plot(rules, method = "matrix", measure = c("support", "confidence"), shading = "lift")

```









